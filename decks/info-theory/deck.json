[
  {"term":"Information source","def":"An information source is a stochastic process that generates symbols according to a probability law over an alphabet. Axiom: information is defined by probabilistic generation, not semantic meaning."},
  {"term":"Alphabet","def":"An alphabet is the finite or countable set of symbols produced by a source. Axiom: all information is encoded as selections from a symbol set."},
  {"term":"Symbol probability","def":"Symbol probability assigns a likelihood to each symbol emitted by a source. Axiom: uncertainty arises from unequal symbol probabilities."},
  {"term":"Information content","def":"The information content of an event with probability p is −log p, measuring surprise in bits. Axiom: rarer events carry more information."},
  {"term":"Shannon entropy","def":"Shannon entropy H = −∑ p log p measures the expected information per symbol from a source. Axiom: entropy quantifies irreducible uncertainty."},
  {"term":"Entropy units","def":"Entropy units depend on the logarithm base, typically bits for base 2. Axiom: information units are scale choices, not structural changes."},
  {"term":"Maximum entropy","def":"Maximum entropy occurs when all symbols are equally probable. Axiom: uniform distributions maximize uncertainty."},
  {"term":"Redundancy","def":"Redundancy is the difference between maximum possible entropy and actual entropy of a source. Axiom: structure appears as reduced entropy."},
  {"term":"Relative redundancy","def":"Relative redundancy is redundancy normalized by maximum entropy. Axiom: efficiency loss is measurable relative to ideal randomness."},
  {"term":"Joint entropy","def":"Joint entropy measures uncertainty of multiple random variables together. Axiom: combined uncertainty reflects shared structure."},
  {"term":"Conditional entropy","def":"Conditional entropy measures remaining uncertainty of one variable given another. Axiom: information reduces uncertainty through dependence."},
  {"term":"Entropy chain rule","def":"The entropy chain rule decomposes joint entropy into conditional entropies. Axiom: complex uncertainty factors into sequential components."},
  {"term":"Statistical independence","def":"Variables are statistically independent if their joint distribution factors into marginals. Axiom: independence implies additive uncertainty."},
  {"term":"Entropy additivity","def":"Entropy is additive for independent sources. Axiom: independent uncertainties sum linearly."},
  {"term":"Information rate","def":"Information rate is entropy per unit time for a stochastic process. Axiom: information flow is entropy normalized by time."},
  {"term":"Discrete source","def":"A discrete source emits symbols from a discrete alphabet with assigned probabilities. Axiom: discrete uncertainty is countable."},
  {"term":"Continuous source","def":"A continuous source emits values from a continuous range, requiring differential entropy. Axiom: continuity alters measurement but not uncertainty logic."},
  {"term":"Differential entropy","def":"Differential entropy extends entropy to continuous variables but is not invariant under coordinate transforms. Axiom: only relative information is meaningful in continuous spaces."},
  {"term":"Choice","def":"A choice is the selection of one outcome from a set of possibilities. Axiom: information arises from constrained choice."},
  {"term":"Message","def":"A message is a sequence of symbols generated by a source. Axiom: information accumulates over symbol sequences."},
  {"term":"Typical sequence","def":"A typical sequence has empirical frequencies close to the source probabilities. Axiom: most long messages look statistically average."},
  {"term":"Asymptotic equipartition property","def":"The AEP states that long sequences concentrate on a typical set of size about 2^{nH}. Axiom: large randomness collapses to predictable structure."},
  {"term":"Typical set","def":"The typical set contains sequences with near-equal probability under the source. Axiom: probability mass concentrates exponentially."},
  {"term":"Coding","def":"Coding maps source symbols to sequences of code symbols for transmission or storage. Axiom: representation transforms but does not create information."},
  {"term":"Prefix code","def":"A prefix code ensures no codeword is a prefix of another, enabling instant decoding. Axiom: decodability requires structural constraints."},
  {"term":"Code length","def":"Code length is the number of symbols used to represent a source symbol. Axiom: efficient codes match length to probability."},
  {"term":"Average code length","def":"Average code length is the expected codeword length under the source distribution. Axiom: performance is measured in expectation."},
  {"term":"Source coding","def":"Source coding compresses data by exploiting statistical structure. Axiom: compression exploits predictability."},
  {"term":"Source coding theorem","def":"The source coding theorem states entropy is the lower bound on average lossless code length. Axiom: entropy limits compression."},
  {"term":"Lossless compression","def":"Lossless compression allows exact reconstruction of the original message. Axiom: perfect recovery requires respecting entropy bounds."},
  {"term":"Noiseless channel","def":"A noiseless channel transmits symbols without error. Axiom: capacity equals symbol rate in absence of noise."},
  {"term":"Channel","def":"A channel is a medium that conveys symbols from sender to receiver, possibly with distortion. Axiom: communication is constrained transformation."},
  {"term":"Channel capacity","def":"Channel capacity is the maximum achievable information rate with arbitrarily low error. Axiom: every channel has a fundamental limit."},
  {"term":"Noisy channel","def":"A noisy channel introduces probabilistic errors into transmission. Axiom: noise converts certainty into uncertainty."},
  {"term":"Equivocation","def":"Equivocation measures remaining uncertainty about the source after observing the channel output. Axiom: noise leaves residual uncertainty."},
  {"term":"Mutual information","def":"Mutual information measures information shared between input and output. Axiom: communication succeeds by reducing uncertainty."},
  {"term":"Channel coding","def":"Channel coding adds redundancy to combat noise during transmission. Axiom: reliability trades off against efficiency."},
  {"term":"Channel coding theorem","def":"The channel coding theorem states reliable communication is possible below channel capacity. Axiom: error-free transmission obeys capacity limits."},
  {"term":"Error probability","def":"Error probability measures the chance of incorrect decoding. Axiom: performance is probabilistic, not absolute."},
  {"term":"Block coding","def":"Block coding encodes sequences of symbols jointly. Axiom: longer context enables better efficiency."},
  {"term":"Markoff process","def":"A Markoff process has future states depending only on the present state. Axiom: limited memory constrains uncertainty."},
  {"term":"Ergodic source","def":"An ergodic source has time averages equal ensemble averages. Axiom: long-run behavior reflects true statistics."},
  {"term":"Stationary process","def":"A stationary process has time-invariant statistical properties. Axiom: stable statistics enable consistent coding."},
  {"term":"Information loss","def":"Information loss occurs when entropy decreases irreversibly under transformation. Axiom: lost uncertainty cannot be recovered."},
  {"term":"Noise","def":"Noise is randomness introduced by the channel independent of the source. Axiom: noise adds entropy without meaning."},
  {"term":"Efficiency","def":"Efficiency is the ratio of entropy to average code length. Axiom: optimal systems approach entropy limits."},
  {"term":"Fundamental limit","def":"A fundamental limit is a bound that no system can exceed regardless of design. Axiom: information obeys mathematical constraints."}
]
