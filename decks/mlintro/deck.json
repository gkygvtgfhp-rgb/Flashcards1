[
{ "term": "Machine Learning", "def": "The study of algorithms that improve task performance through experience by fitting functions to data and generalizing beyond observed samples. Axiom: Learning is optimization under uncertainty guided by data and bias." },
{ "term": "Concept Learning", "def": "The problem of inferring a Boolean-valued target function that classifies instances as members or non-members of a concept from labeled examples. Axiom: Concepts are functions inferred from finite evidence." },
{ "term": "General-to-Specific Ordering", "def": "A partial order over hypotheses where one hypothesis is more general if it classifies a superset of instances compared to another. Axiom: Hypothesis spaces admit structural orderings that constrain search." },
{ "term": "Inductive Bias", "def": "The set of assumptions a learner uses to generalize from limited data to unseen cases, implicitly restricting the hypothesis space. Axiom: Generalization requires bias." },
{ "term": "Decision Tree Learning", "def": "A learning method that represents hypotheses as trees by recursively partitioning the instance space using feature-based tests. Axiom: Hierarchical feature tests can approximate complex decision boundaries." },
{ "term": "Overfitting", "def": "A failure mode where a hypothesis fits noise or idiosyncrasies in training data, leading to poor generalization on new data. Principle: Fit quality must be evaluated relative to generalization, not memorization." },
{ "term": "Occams Razor (Machine Learning)", "def": "The preference for simpler hypotheses among those consistent with the data, motivated by improved generalization bounds. Principle: Simplicity regularizes search." },
{ "term": "Artificial Neural Network", "def": "A parameterized function composed of layers of weighted units that compute nonlinear transformations of inputs. Axiom: Distributed representations enable nonlinear approximation." },
{ "term": "Backpropagation", "def": "An efficient algorithm for computing gradients of a loss function with respect to network parameters using the chain rule. Axiom: Credit assignment follows gradient flow." },
{ "term": "Gradient Descent", "def": "An iterative optimization method that updates parameters in the direction of the negative gradient of a loss function. Axiom: Local gradient information guides global improvement." },
{ "term": "Estimation Theory", "def": "The framework for inferring model parameters from noisy samples while balancing bias, variance, and sample efficiency. Axiom: Finite data implies probabilistic inference." },
{ "term": "Hypothesis Accuracy", "def": "An estimate of how closely a learned hypothesis approximates the true target function over the data distribution. Axiom: Performance must be defined statistically, not absolutely." },
{ "term": "Confidence Interval", "def": "A probabilistic bound on an estimated quantity that quantifies uncertainty due to finite sampling. Axiom: Learning outcomes require uncertainty quantification." },
{ "term": "Statistical Comparison of Learners", "def": "Methods for determining whether observed performance differences between learning algorithms are statistically significant. Principle: Performance claims require statistical evidence." },
{ "term": "Bayesian Learning", "def": "A learning framework that maintains and updates a posterior distribution over hypotheses using Bayes theorem. Axiom: Learning is belief revision under probability." },
{ "term": "Bayesian Algorithm", "def": "A learning algorithm that explicitly represents uncertainty using probability distributions over hypotheses or parameters. Axiom: Uncertainty is a first-class object in inference." },
{ "term": "Naive Bayes Classifier", "def": "A probabilistic classifier that applies Bayes theorem under the assumption of conditional independence among features. Principle: Strong assumptions can yield tractable inference." },
{ "term": "Computational Learning Theory", "def": "A formal framework analyzing what can be learned efficiently given constraints on data, computation, and error. Axiom: Learnability is a property of both data and algorithms." },
{ "term": "PAC Learning", "def": "A model in which a learner must, with high probability, output a hypothesis whose error is within a specified bound of optimal given enough samples. Axiom: Learning guarantees are probabilistic and asymptotic." },
{ "term": "Mistake-Bound Model", "def": "An online learning framework that bounds the total number of prediction errors a learner can make before convergence. Axiom: Learning progress can be measured by errors over time." },
{ "term": "Weighted Majority Algorithm", "def": "An online ensemble method that combines expert predictions by maintaining and updating weights based on past performance. Axiom: Aggregation reduces regret." },
{ "term": "Instance-Based Learning", "def": "A learning paradigm that defers generalization until query time by comparing new instances to stored examples. Axiom: Memory can substitute for explicit models." },
{ "term": "k-Nearest Neighbors", "def": "An instance-based algorithm that predicts outputs based on the labels of the k most similar stored examples. Principle: Local similarity induces local consistency." },
{ "term": "Locally Weighted Regression", "def": "A method that fits a simple model around a query point using nearby examples weighted by distance. Axiom: Global complexity can be approximated locally." },
{ "term": "Case-Based Reasoning", "def": "Problem solving by retrieving and adapting solutions from similar past cases. Axiom: Experience is reusable structure." },
{ "term": "Genetic Algorithm", "def": "A stochastic search method that evolves a population of candidate solutions using selection, mutation, and recombination. Axiom: Variation plus selection drives optimization." },
{ "term": "Genetic Programming", "def": "An evolutionary approach that evolves executable programs or symbolic expressions as solutions. Axiom: Programs are searchable objects." },
{ "term": "Inductive Logic Programming", "def": "A learning framework that induces logical rules from relational data using background knowledge. Axiom: Structure enables relational generalization." },
{ "term": "Explanation-Based Learning", "def": "A deductive method that generalizes from a single example by explaining it using a domain theory. Principle: Explanation can replace enumeration." },
{ "term": "Bias-Plus-Data Learning", "def": "An approach that combines prior structural knowledge with empirical data to improve learning efficiency. Axiom: Prior knowledge constrains viable hypotheses." },
{ "term": "Reinforcement Learning", "def": "A learning paradigm where an agent learns a policy by interacting with an environment to maximize cumulative reward. Axiom: Value emerges from delayed consequences." },
{ "term": "Delayed Reward", "def": "A setting in which feedback is temporally separated from actions, requiring credit assignment mechanisms. Axiom: Temporal structure complicates causality." },
{ "term": "Well-Specified Learning Problem", "def": "A learning setup defined by a task, a performance measure, and a source of experience. Axiom: Learning requires explicit objectives." },
{ "term": "Learning as Search", "def": "The view that learning consists of searching a hypothesis space guided by data, bias, and evaluation metrics. Axiom: Learning is constrained exploration." }
]
